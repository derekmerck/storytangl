# MVP Progress Review
11/13/25

## Core Graph and Entity System

**Completed and Working Well:** The fundamental graph engine is robust. Core abstractions like **Entity**, **Graph**, **Node**, and **Edge** are implemented with clear semantics and strong immutability guarantees. Every entity has a stable UUID identity and is managed in registries for quick lookup. Graph topology operations (adding nodes/edges, nesting subgraphs) are solid, with support for hierarchy (e.g. `Subgraph.parent` caching) and tagging. The design enforces strict identity/reference separation -- graph-linked objects store only UUID references and resolve via their graph on access. This ensures serialization is clean (only IDs in structured output) and that all pointer dereferences funnel through registries (enabling watchers for audit). The event/record subsystem is another strength: any state change produces immutable **Record** objects (Events, Patches, Snapshots) for full audit trails and replay. Records are sequenced in a **StreamRegistry** to preserve a linear history and allow slicing by time or channel. The combination of **Snapshot** + **Patch** supports deterministic restore of prior states. These mechanisms give the engine a solid foundation for reproducibility and debugging. Core code is generally well-tested (graph ops, record round-trips, etc.), reflecting the emphasis on deterministic behavior and **Pydantic** models for consistent serialization.

**Gaps and Issues:** A few low-level quirks need attention. **Auto-registration of GraphItems** can be confusing -- currently, constructing a Node with a `graph` argument auto-attaches it to the graph via a validator, which can surprise tests (an unattached Node might still appear in a graph registry). The guidance is to prefer explicit `graph.add(node)` instead, and it's noted this auto-add behavior might be removed in the future. There are minor consistency questions around **Edge mutability**: edges allow their `source`/`destination` to be reassigned after creation, which could violate graph invariants if misused. It's not documented whether edges should be immutable once added; clarifying or restricting this would improve safety. Another subtle point is the caching of `Subgraph.parent` -- it uses a cached property for performance, but requires manual invalidation when re-parenting nodes. The engine does call `_invalidate_parent_attr()` on moves, but this pattern is error-prone. Similarly, the `Entity.is_dirty` flag exists to mark non-deterministic mutations, but nothing auto-sets it yet. This means truly "tainted" state (like eval'ed code or forced jumps) might not be flagged unless developers manually mark it -- an opportunity to integrate with the registry/watchers for automatic dirty marking. These are relatively small issues, but they hint at places to refine the core for clarity. On the data model side, all entities currently rely on Pydantic's base model; the project's custom `BaseModelPlus` (with extras like unified UID handling) is used consistently, which is good. One caution: the interplay of custom `structure()` overrides and Pydantic can cause recursion if not done carefully (a known pitfall with an outlined solution). Ensuring that new entity subclasses follow the hook pattern (calling super().structure first) is important to avoid infinite loops in serialization.

**Recommendations:** Clean up the minor abstraction wrinkles to strengthen the layer's cohesion. For example, consider removing the implicit auto-registration of entities on creation -- it's safer to force explicit graph attachment to avoid hidden side effects (the docs already recommend this). If edge mutability is not a desired feature, enforce edges as read-only once added (or document the expected use if runtime retargeting is needed). The parent caching might be simplified by eliminating the cache or using a less error-prone strategy, since a small performance hit is preferable to subtle state bugs. Integrating the `is_dirty` flag with the core dispatch or registry layer would improve **auditability** -- e.g. automatically mark the graph dirty on any direct `eval`/exec calls or non-replayable mutations. This would fulfill the intent that "mutations become artifacts" and nothing sneaks by unnoticed. In general, the core is strong; the above changes are mostly polish. Another medium-term refactor to consider: unify any parallel concepts between core and higher layers. For instance, the **StoryGraph** vs core **Graph** distinction appears minimal -- if `StoryGraph` is just a thin wrapper, it might be unnecessary duplication. Ensuring the core graph model is generic enough (which it seems to be) means story-specific needs can be handled via composition or subclasses without forking core logic. The goal should be a single authoritative graph implementation. Finally, continue to enforce the layering discipline: core should remain free of any domain/story logic or higher-level imports (currently it does -- e.g. story domain objects inherit core `Node` rather than core depending on story, which is correct). Regular audits for any accidental upward imports or hidden globals will keep the core engine clean.

## Planning and Provisioning (VM Layer)

**Completed and Working Well:** The planning/provisioning system is one of the most advanced parts of the engine, and much of its intended functionality is in place. The VM defines a multi-phase **resolution cycle**, with a dedicated PLANNING phase (and matching FINALIZE phase) to anticipate next steps in the story before the user makes a choice. The concept of a **frontier** (the set of next reachable nodes) is implemented -- `_iter_frontier` correctly gathers all upcoming choice destinations from the current cursor node. For each frontier node, the engine can compute **Requirements** (what that node needs) and attempt to satisfy them via various **Provisioners**. All four main provisioning strategies are implemented: searching for an existing resource, creating a new one from a template, updating a resource, or cloning one. The Provisioner classes -- `GraphProvisioner`, `TemplateProvisioner`, `UpdatingProvisioner`, `CloningProvisioner`, plus a special-case `CompanionProvisioner` -- are present and integrated into the planning pipeline. The selection of offers is deterministic and cost-based: the engine generates **Offer** objects for each possible way to fulfill a requirement and then picks the best offer by cost, proximity, etc., which is all working (the `_select_best_offer` logic and offer deduplication are done). Crucially, planning runs *before* the narrative advances, so by the time choices are presented, the system has already provisioned any needed actors, items, or other resources -- preventing the dreaded "softlock" where a choice leads nowhere. In fact, **softlock detection** is built-in: if the planner finds no viable way to satisfy a required dependency on any frontier node, it flags it (currently by simply not including that choice). The planner also distinguishes **hard requirements** (must be met for a choice to be available) versus soft ones (optional or nice-to-have), enforcing gating appropriately. All this is supported by a solid data model: requirements carry a `hard_requirement` flag, criteria for matching, optional template data, and a chosen policy, and these are attached to special graph edges (**Dependency** for "needs X" and **Affordance** for "can appear here") which the planner knows how to resolve. The integration with the VM runtime is also well done -- the Frame/Context in the VM invokes planning at the right time, caches the computed plan, and then the FINALIZE phase applies the chosen provisions to the graph (e.g. actually adding the new item or linking an existing character). The result of each planning cycle is logged as a **PlanningReceipt**/**BuildReceipt** artifact, so the ledger records what was provisioned or why nothing was (a big win for debugging and narrative audit). In summary, the provisioning core -- frontier detection, requirement modeling, multi-strategy fulfillment, and event-sourced application -- is largely complete and aligns well with the project's dynamic narrative goals.

**Gaps and Issues:** A few important pieces of the planning system remain unfinished or only partially integrated, reducing its current power. The highest priority gap is the **integration of story scripts with provisioning**: specifically, when a World script defines roles or settings for a scene (e.g. a scene that requires a "bartender" actor or an "interior" location), those are parsed but currently **ignored** when building the story graph. In the `World.create_story` logic, scene roles/settings should translate to Dependency edges on the scene node (with Requirements for an actor or location), but this wiring isn't implemented yet. Consequently, dynamic casting of roles (like populating a scene with a referenced or newly created actor) doesn't happen -- all scenes start with roles unfilled. Tests confirm this oversight: a scene with a role "king" should result in a Role node that either stays empty (if no actor provided) or links to an actor if referenced, and mark the role's `satisfied` flag appropriately. Right now, the engine does create the Role node (the test finds one), but it only attaches the actor if the actor was explicitly defined with the same label -- there's no **Dependency edge** to formally connect them via the planner (the association is done at story creation in a rudimentary way). The design blueprint even includes sample code to fix this in `World.create_story` by instantiating a Requirement and a Dependency edge for each role. Not implementing this means the powerful provisioning logic (Policies like ANY vs CREATE) isn't actually used for script-defined roles; it's a manual or no-op process right now. A related gap is in the **TemplateProvisioner**: the system allows script authors to define reusable templates (e.g. a template for a "golden_key" item) and then reference them in requirements, but currently the TemplateProvisioner doesn't know how to resolve a `template_ref`. Only inline templates (embedded in the requirement) work. This is acknowledged as not done -- the TemplateProvisioner needs access to a template registry (likely provided by the ScriptManager or World) so that if a requirement says `template_ref="golden_key"`, it can look up the actual template data. Without this, script-defined templates in the YAML are useless. Another issue is how **choice availability** is conveyed. Currently, if a frontier node can't be provisioned (hard requirement unsatisfied), the engine simply filters out that choice entirely. This prevents softlocks, but it means the UI/user is unaware that a choice existed but was unavailable. The plan is to mark choices with an `available=False` flag and a reason (like "Missing required key"), so the UI could, for example, gray out the choice or show a tooltip. That metadata isn't implemented: it requires adding fields to ChoiceEdge and adjusting `Frame.get_available_choices()` to include unavailable ones with flags. Until that's done, the engine's approach of silently dropping choices is a responsibility leak (the engine is effectively deciding a UI policy by hiding content). Beyond these high-priority fixes, there are some advanced features planned but not done: **multi-tier lookahead** (the planner currently validates only the immediate next step, but ideally it could look 2 or more steps ahead to avoid dead-ends further out), and **resource lifecycle management** like garbage-collecting unused provisioned nodes or expiring "Affordance" offers that were created but never used. At present, anything the planner creates stays in the graph indefinitely, which could become a memory leak or cause narrative clutter over long sessions. Episode-scoped provisioning (cleaning up resources at end of an episode) is also noted as missing. These are medium-term enhancements -- not critical for functionality, but important for narrative consistency and performance as stories grow.

**Recommendations:** The immediate steps are clear from the above gaps. **Integrate role and setting requirements during world compilation** -- this means enhancing `World.create_story` (or the Scene model) to attach `Role` and `Setting` edges with proper Requirement objects referencing either a specific actor/location or a criteria/tag. The code snippet in the planning design document can serve as a guide. This will unlock dynamic casting: the next time the planner runs (immediately at story start, since the scene with an open Role is on the frontier), it will provision those roles. In practice, that means if a script says a scene needs an actor with identifier "king_actor", the system will either bind the existing character or create a new one according to the policy. Similarly, implement the **TemplateProvisioner lookup**: pass the world's template dictionary into the planner (perhaps via the ProvisioningContext or have ScriptManager inject it into each Requirement as it's created). The fix is outlined in code -- this is relatively straightforward and will allow authors to define templates once and reuse them across requirements. For choice availability, modifying the ChoiceEdge model to include `available: bool` and an optional `reason_unavailable` and adjusting the filtering logic will better separate engine logic from presentation. The engine should list all choices, marking those that are blocked so the UI can decide how to display them (instead of the engine dropping them entirely). This change will require propagating that info through the service/CLI layers (e.g., the CLI could then list choices with "\[locked\]" notation, and the web UI can disable buttons). After these fixes, turn attention to the longer-term improvements: implementing a **tiered planning depth** (perhaps as an optional mode or debug tool, since full subtree validation can be expensive) to catch dead ends that are a couple of steps out. Likewise, add hooks for **resource cleanup** -- maybe a background process or a FINALIZE-phase handler that removes or archives unused nodes after a certain scope ends (e.g. when leaving an episode, remove any provisioned characters that never got used). These can build on the existing event/receipt system: e.g., track which provisioned nodes were actually traversed or interacted with, and prune those that were not. Another recommendation is to bolster test coverage around provisioning. Unit tests for scenarios like "missing hard requirement triggers unavailable flag" or "template_ref resolves correctly" will prevent regressions. Given the complexity of this subsystem, also consider logging or exposing the plan results in debug mode -- e.g., a way to query "why is this choice unavailable?" which could return the unsatisfied requirement. This would be invaluable for authors play-testing their worlds. Finally, maintain the clear separation of concerns: keep pure provisioning logic in the **Provision Layer** (currently `tangl.vm.provision` does this well) and only orchestration in the **VM/Frame layer**. The current design is sound: the VM frame gathers open edges and asks each provisioner for offers. Sticking to this pattern will make it easier to add new provisioner types (like a network provisioner for online content, etc.) without touching the core planner. In short, focus on wiring up the last mile between the content definitions and the provisioning engine, then gradually enhance lookahead and cleanup policies to make the system truly robust.

## Journaling and Presentation Output

**Completed and Working Well:** StoryTangl already produces a linear **journal** of narrative output as the story runs, which is key for presenting the story to users. The design uses an immutable stream of **Fragment** records to represent all narrative text and media outputs in sequence. Every time the engine "advances" the story or performs an action, it emits fragments (pieces of content) that get appended to a journal log (the Ledger's record stream). The core of this is the multi-handler **JOURNAL phase** in the dispatch system. Notably, the `Block` node (the fundamental interactive unit of a scene) orchestrates three ordered journal handlers: an EARLY handler to render the block's own prose content, a NORMAL handler to collect any dynamically described content from child concepts, and a LATE handler to generate the available choice prompts. This sequence (content → concepts → choices) is enforced by handler priority and works well -- it ensures that when a block is the "current cursor," you first get the block's text, then any descriptions of characters/items in the scene, then finally the list of choices for what to do next. The rendering of text is cleanly separated into a two-stage pipeline: raw text templates are rendered to plain strings (with a Jinja2-based **ContentRenderer**), and then those strings are wrapped in Fragment objects for output. This keeps presentation concerns modular -- e.g., a block's content might be `"You are in the {{location_name}}"` and the renderer fills in the `location_name` from context before the fragment is created. The fragment objects themselves (instances of `BaseFragment` or its subclasses) carry structured data: at minimum, a `content` field (could be text or media), a `fragment_type` (to hint how it should be treated, like `"text"` vs `"choice"`), and references back to the source entity (`source_id`, etc.) for traceability. This structured approach is a strength: even though fragments are output as JSON to clients, they aren't just raw strings -- they include metadata that a UI can use (for example, knowing a fragment is of type \"choice\" vs \"dialog\"). The journaling system also cleanly distinguishes between **replayable events** and **non-replayable narrative**. Events (which mutate state) go into the event log, while Fragments (which represent how things are presented to the player) go into the journal. This means the same underlying event (like an "attack" action) could produce different fragment text if re-run under different conditions, but the event log stays consistent -- a good separation for debugging. The code structure under `tangl.journal` and `tangl.story` includes base classes for fragments and specific fragment types (Content vs Control vs Info), showing an intent to categorize output further. Another positive is that the service layer is already exposing journal data: for example, the `RuntimeController.get_journal_entries` endpoint returns a list of recent fragments for the client, and the CLI has a "journal" command to show the latest story text. This confirms that the journaling mechanism is sufficiently implemented to be used in both text and web interfaces. The **two-stage render** architecture (first stage: pure text generation, second stage: wrapping in domain-specific fragments) is in place and aligns with the project philosophy of keeping content rendering logic separate from UI formatting. Overall, the engine does produce the needed narrative output in a linear, timestamped sequence, which is the foundation for any presentation layer (CLI, web, etc.) to build on.

**Gaps and Issues:** While the journaling concept is implemented, there are areas where it's only superficially wired up, and opportunities to make the output more semantically rich are not yet fully realized. One issue is that the engine currently returns fragments mostly as generic **BaseFragment** objects with minimal typing, rather than using the specialized fragment subclasses for richer structure. For example, when choices are generated, the `Action.choice_fragment()` method just creates a BaseFragment of type \"choice\" with content and maybe a payload. There is a defined `ChoiceFragment` class (in `tangl.journal.discourse`) that includes an `active` flag (to denote whether a choice is currently selectable) and an `activation_payload` for passing data when the choice is picked -- but the engine isn't using it yet. The `BlockFragment` class is similarly defined to hold a block's content plus a list of child choice fragments, but in the current flow, the block's text and the choices come out as separate entries in the fragment list, not as one nested structure. In fact, the code has a TODO noting that the Block fragment "expects a list of choice fragments as a parameter" and that this could be considered later. So, the output stream right now is a flat list of fragments in chronological order, whereas the intended design could allow grouping (a BlockFragment containing its choices). This flat output works, but it means some semantic info is lost -- the client has to infer which choice fragments go with which content block based on sequence. Another related issue is that **unavailable choices are not represented in the journal at all** (as discussed in the planning section). The fragment stream doesn't include "this choice was present but locked" markers because the engine filters them out entirely. Ideally, a choice fragment could be output with an `active=False` flag (the `ChoiceFragment.active` field exists for this) so that the UI could show a grayed-out option. Not having this means the journal omits certain narrative possibilities that the player might otherwise be hinted about. Additionally, the integration of **media content** into the journal is only partially done. The engine has a concept of `MediaFragment` (with a MediaRIT content pointer) and the planning system can provision media, but currently there's no handler actually emitting those MediaFragments during JOURNAL phase -- the design doc notes that Block's journal handlers should emit media fragments for any satisfied MediaDependencies, but this code is missing (the example is given in the media design spec, not in the live code). So, if a story block has an image or sound attached, the planner might resolve the media resource, but the player won't see it because no fragment is output (effectively a silent failure to present). Likewise, on the client side, the service doesn't yet translate MediaRITs into URLs or data -- the `RuntimeController.get_state` in the media design doc shows a stub where it would swap MediaFragments' content with actual URLs/base64 before responding, but in practice `get_journal_entries` currently just dumps whatever fragments it has (likely with raw RIT objects that the client wouldn't know how to handle). Another gap is the general **formatting and envelope of responses**. The service returns fragments as JSON by serializing the Pydantic models, but as noted in the service layer review, there's no unified response schema -- different endpoints return different shapes. For the journal specifically, the `/update` REST endpoint currently returns `{"fragments": [...], "choices": [...]}` as two separate lists. This means the client has to merge the narrative text and the interactive choices on its side. It works (the web client does exactly this, building `JournalStoryUpdate` objects from those lists), but it's not as clean as sending down already combined "story update" blocks. The existence of concepts like `JournalStoryUpdate` and `StoryBlock` on the frontend indicates the client is expecting a structure where a block has text, maybe media, and an array of actions. The backend could do this assembly but currently does not. Additionally, features like **control fragments** (for out-of-narrative signals, UI instructions, etc.) are barely touched -- the `journal.notes.md` hints at "control fragment" types (grouping, UI signals), but we don't see these being produced in code. This is likely just not needed yet, but as the presentation layer grows (say, to handle scene breaks or UI events like "fade to black"), the engine might need to emit such control fragments. In summary, the journaling system's skeleton is in place, but some muscle is missing: rich fragment types exist in theory but not fully utilized, media and availability statuses aren't flowing through, and the packaging of output for clients could be more coherent.

**Recommendations:** The journaling and presentation layer would benefit from a pass to enrich and standardize the output, in tandem with minor adjustments to how clients consume it. First, **begin using the specialized Fragment subclasses** that have already been defined. For instance, when creating choice fragments, instantiate a `ChoiceFragment` (with `active=True/False` as appropriate and carry over any `payload` as `activation_payload`) instead of a raw BaseFragment. This will likely require adjusting the return types in the handlers (they currently return `BaseFragment` or list thereof; returning the subclass which inherits BaseFragment should be fine). Similarly, wrap the block content and its choices together: the Block's EARLY handler could return a `BlockFragment(content=..., choices=[...])` containing the results of the LATE handler, rather than the current approach of returning content alone and letting choices come separately. Achieving this might involve refactoring how the dispatch collects handler outputs (since currently each handler returns independently). One approach: have the Block's journal handlers construct fragments but not push them directly; instead, aggregate them in the Context and output a final combined fragment at LATE priority. Alternatively, after collecting the fragment list, the service layer could post-process and group a BlockFragment with its subsequent ChoiceFragments. The goal is to send to the client a structured story update chunk (one block with its choices nested). This ties into the next suggestion: define a **unified response schema for story updates**. For example, a `JournalStoryUpdate` could be a dict with `{text: "...", media: [...], actions: [...]}` corresponding to one story beat. The backend can translate its fragment list into this schema before sending. This would dramatically simplify client logic (and avoid the timing issues of separate "fragments" vs "choices" calls). The FastAPI router's `_serialize()` helper could be extended or replaced with a specific function to build this structure from the raw fragments and choice list. Aligning the output with the frontend's expected types (as seen in the Vue code) will ensure the web UI and engine don't drift in understanding. On the **media integration**: once the above grouping is in place, implement the missing Journal handler for media. The design suggests adding a NORMAL-priority handler on Block that iterates `self.edges_out(is_instance=MediaDependency)` and for each satisfied one, creates a `MediaFragment` with the appropriate role and hints. This should be straightforward now that the media provisioning exists; the handler can be registered similarly to how concept descriptions are. Then, the service layer should **dereference MediaFragments** on output -- i.e., replace the internal RIT (resource tag) with either a URL or base64 data depending on config. This likely involves extending `Orchestrator.execute` or the controller method to detect MediaFragments in the result and transform them. It's noted as missing, and implementing it will allow images and other media to actually show up for the player. Another recommendation is to revisit how **info/status fragments** are delivered. Currently `get_story_info` provides some summary of the ledger state, and the web client uses it for a sidebar. Potentially, the engine could emit "info fragments" (of type `info` or similar) at certain milestones (like a fragment containing current score, or chapter title) rather than requiring separate calls. This isn't critical now, but planning for it keeps the design consistent (one linear stream of all story output, not some in stream and some out-of-band). In terms of testability and extensibility: add tests for the journaling output format. For example, after a choice resolution, assert that the fragments returned include the expected text and choice entries. If introducing BlockFragment grouping, test that one BlockFragment contains its choices. As the output format solidifies, updating documentation is important -- the README or docs should specify what the client can expect from `get_journal_entries` or the new unified endpoint (especially if switching to a single "story update" object). Finally, consider the **presentation hints** system: fragments have a `presentation_hints` field (for styling, e.g. "italicize this" or "show with a delay"). This is an area to expand once basics are done -- define a vocabulary of hints and ensure the engine populates them where appropriate (for example, a fragment coming from a character's dialog node might carry a hint like `{"css_class": "dialog"}` or a special icon). The heavy lifting would be on the UI side to interpret hints, but establishing the pipeline now (even as a no-op pass-through) will make the system more flexible for future narrative presentation features. In summary, by leveraging the rich fragment structures already conceived and packaging the journal in a more coherent way for clients, the engine will provide a much cleaner and more powerful interface for the presentation layer to create immersive experiences.

## Orchestrator and Service Layer

**Completed and Working Well:** The service layer (v3.7) is a clear architectural win -- it provides a firm boundary between the engine internals and any external interface (CLI, REST, etc.). The centerpiece is the **Orchestrator**, which is fully implemented and handles the core responsibilities: it registers all available controller endpoints, hydrates the required resources (like loading the User, their current Ledger, and a new Frame for each request), and invokes the appropriate controller method. The orchestrator's dependency-injection by type hint is working well -- controller methods just declare their parameters (e.g. a parameter of type `Ledger` or `User`), and the orchestrator automatically provides the correct instance based on the `user_id` or other ids passed in. This means application code never manipulates engine objects directly, which is a major safety and maintainability boon (no bypassing the engine invariants). All four core controllers are in place: **RuntimeController** (for story-play operations), **WorldController** (for world catalog and loading), **UserController** (user accounts), and **SystemController** (health/diagnostics). These cover the essential API surface. For example, RuntimeController provides endpoints to create a story, get choices, advance a choice, fetch journal entries, jump the cursor (for debug), etc., which maps well to what a client needs. WorldController can list available worlds and load new world content from script, and indeed `WorldController.load_world` uses the ScriptManager to ingest YAML -- that path is implemented and working, allowing new story worlds to be added at runtime. Persistence integration in the service layer is especially strong: there's an abstract **PersistenceManager** with multiple backend implementations (in-memory, JSON/YAML file, Redis, MongoDB) all wired in. The orchestrator uses this to load and save **User**, **Ledger**, etc. transparently. The context-manager pattern for PersistenceManager is in place, so changes to a Ledger during a request can be automatically saved back at the end without manual calls. This design supports easy swapping of persistence layers (for example, running tests with an in-memory store vs. a production MongoDB) without affecting the engine code. Another notable service feature is the **ApiEndpoint** decorator which annotates controller methods with metadata like access level (public/user/admin) and method type (READ/CREATE/UPDATE/DELETE). The implementation infers some of this automatically (e.g. naming conventions for method type) and even classifies endpoints by response content type. This metadata system is mostly complete and it's future-proofing the service for things like documentation and access control. The orchestrator indeed collects these annotations when registering controllers. The application layer integration (CLI and REST) is also in a good state. The **CLI** is built on `cmd2` and includes commands for all major flows: logging in as a user, loading worlds, starting stories, making choices, and viewing the journal. The CLI uses the orchestrator under the hood for all operations, meaning it benefits from the same validation and lifecycle management (for instance, it can't accidentally bypass persistence). The **REST API** is built with FastAPI and likewise delegates to orchestrator calls for each route. The REST layer has implemented per-user concurrency locks (to prevent race conditions if a user triggers two actions at once). It also uses API keys for auth (a simple API key -\> user mapping), which is enough for the current stage. Impressively, the FastAPI integration automatically provides an OpenAPI doc -- so the API is self-documenting to an extent. The structure of splitting routes by concerns (story, world, user, etc.) and using Pydantic models for request/response (e.g. `ChoiceRequest` model in the story router) makes the external API relatively clean. In summary, the service layer successfully shields the engine: external clients call a consistent API (e.g. "*RuntimeController.resolve_choice*"), and under the hood the orchestrator ensures the correct ledger/frame is mutated and saved, with all errors or validations funneled through one place. This decoupling has already paid off -- for example, the engine's internals (like how a Ledger advances) can change without changing the REST or CLI interface at all, as long as the orchestrator and controllers adapt, which is exactly the intended benefit.

**Gaps and Issues:** The service layer is functionally in place, but there are several areas marked as "to-do" to reach production quality and some opportunities to tighten the design. One notable gap is **enforcing access control**. The ApiEndpoint decorator lets methods declare `AccessLevel.PUBLIC/USER/ADMIN`, but currently this is not actually checked at runtime. In practice, any caller with orchestrator access can invoke even admin endpoints. For example, `UserController.create_user` might be intended as ADMIN-only, but nothing in Orchestrator.execute verifies the caller's role. The plan is to inject an `auth_provider` or use the user context to enforce that (i.e., if a user token with only PUBLIC rights calls a USER endpoint, deny it). Right now, that's not done -- likely okay in a dev environment (where you trust yourself), but not safe long-term. Another weakness is **error handling consistency**. Some controller methods raise generic `ValueError` or `KeyError` with minimal messages, which the REST API turns into HTTP 400 or 500, but there's no systematic distinction between client errors (bad request) and server errors (bug). Nor is there a unified error response format. The absence of a custom exception hierarchy (e.g. `StoryTanglError` base with subclasses like `NotFoundError`, `InvalidInputError`) means the service might leak Python exceptions or send unclear messages. The design notes call for creating such an exception hierarchy and mapping them to proper HTTP status codes. Similarly, results coming out of controllers are not standardized. For instance, some endpoints return a dict with a `"status": "ok"` field, others return raw data or Pydantic models which serialize to JSON differently. In the current REST routes, we see ad-hoc fixes, like after calling `RuntimeController.create_story`, the router manually removes the `ledger` object from the result and saves it, then returns the rest as a dict. This indicates the controller was returning a mix of data (perhaps a Ledger or User object) that isn't directly JSON serializable. Unifying responses in a **BaseResponse** (with `{ success: bool, data: ..., error: ... }`) is on the to-do list. This isn't just cosmetic; it will simplify client-side handling and ensure errors are caught and reported in a structured way. Another feature incomplete is **endpoint introspection and documentation**. While FastAPI's docs exist, there's no programmatic way to list available orchestrator endpoints or get their metadata from within the system. Having something like `orchestrator.list_endpoints()` or an Admin endpoint that outputs all registered API operations (with descriptions) would be useful -- it's mentioned but not done. The groundwork with ApiEndpoint metadata is there, so this is more of an enhancement. Regarding **preprocessors/postprocessors**: the framework supports them (e.g., `preprocessors=[_dereference_world_id]` as used in WorldController methods to translate a world_id to a World object), but there are no real examples of complex usage yet. This is fine (they're not strictly necessary), but it means cross-cutting concerns like logging or input validation haven't been fully explored via these hooks. Multi-tenancy is another forward-looking item: currently one orchestrator == one persistence context (and essentially one "tenant"). The system can't segregate data by tenant except by running separate orchestrator instances. That's noted as a limitation. In practice, this means if StoryTangl were running as a service for multiple games or groups, all user and world data goes into one bucket. Lastly, **session management** beyond API keys is not in place. There's no support for session expiration or refresh; the API key approach is static and there's no login flow (the "secret" is just a UUID per user). That's acceptable for now, but would need to evolve for a real deployment (e.g., JWTs or OAuth). These issues don't break functionality in a dev setting, but they mark where the service layer needs refinement to be secure, maintainable, and easy to use for integrators.

**Recommendations:** The service layer is in a good state structurally, so improvements should focus on hardening and polishing features. Top priority for a production environment is **access control**. Implement the enforcement in Orchestrator.execute or perhaps in a global request middleware for REST. One simple approach: have the Orchestrator know the current user's roles or access level (for CLI, everything could be ADMIN by default; for REST, derive from the API key whether the user is admin or not), then when executing, look up the endpoint's required AccessLevel (stored by ApiEndpoint) and raise a PermissionError if the user doesn't qualify. This ties into session management -- if implementing roles/permissions, also implement token-based auth or extend the API key to carry roles. In parallel, introduce a **standard error model**. Define custom exceptions (e.g., `ClientError` with a `.message` and `.status_code`). Map these in the FastAPI exception handlers so they return `{"error": "...message...", "code": ...}` and use appropriate HTTP codes. Controller code can then do `raise ResourceNotFound("No such world")` instead of `ValueError("World X not found")` -- making the intent clear and allowing orchestration to distinguish between a 404 vs a 500. Coupled with that, adopt the **BaseResponse** envelope for all controller returns. This could be implemented by modifying Orchestrator.execute: if the returned value is not already in a standardized form, wrap it. For example, always output `{"success": True, "data": <result>}` and on exception output `{"success": False, "error": str(exc)}` (with the orchestrator catching exceptions). This change will ripple through tests and client code but will greatly normalize interactions. Next, leverage the **introspection** capability latent in ApiEndpoint: provide a `SystemController.list_endpoints` or similar that returns the list of all endpoints with their names, required params, etc. Given the controllers are already discoverable via the HasApiEndpoints mixin, this is mostly formatting that info. It will help with dynamic client generation and debugging (and could feed into automated docs or CLI help). The orchestrator could also generate an OpenAPI schema on the fly from this metadata if needed (though FastAPI already has one, a programmatic listing might still be useful). For **pre/postprocessors**, consider adding at least one example usage to validate the design. A good candidate is input validation as a preprocessor -- e.g., a preprocessor on `resolve_choice` could check that the choice_id is indeed present in the user's current available choices and raise a clean error if not (currently, if you pass a bad UUID, it likely fails deep in the engine). Another could be a simple logging preprocessor that logs every API call with its params for audit. Demonstrating these patterns will both ensure the hooks work as expected and provide templates for future developers. On multi-tenancy, a short-term workaround is possibly to support namespacing in the PersistenceManager (like prefixing all keys with a tenant id). If this is not urgent, it can wait until a real use-case arises, but keeping the design in mind (e.g., don't assume a single global user namespace in all code) is wise. For **session management**, possibly integrate with FastAPI's dependency system to verify API keys on each call (this is already partly done with `uuid_for_key()` usage) and start thinking about a more robust scheme (even if just rotating API keys or adding expiration). Another area: ensure that the **service layer logs** enough. The orchestrator currently likely logs warnings and errors at least, but adding info-level logs for each execute (like "User X called YController.method") with maybe correlation IDs (user id or request id) would help trace issues in a deployed setting. Finally, continue writing integration tests for the service layer: e.g., simulate a full flow (create user, load world, start story, make choice) via orchestrator or even via the FastAPI test client. Some integration tests exist, and expanding them will catch any inconsistencies introduced by changes in controllers or orchestrator logic. In conclusion, the service layer needs mostly incremental improvements. Its architecture -- a clear orchestrator/endpoint split -- should remain as is, since it's enabling all the good decoupling. The main goal now is to make the layer "enterprise-ready" by tightening security, consistency, and introspectability. This will pave the way for confidently exposing StoryTangl as a service to real clients.

## Application Layer (CLI and API Interfaces)

**Completed and Working Well:** On top of the service layer, StoryTangl provides a CLI and a REST API (with a prospective GUI client) that demonstrate the engine in action. The **CLI** (`tangl.cli`) is functional and supports interactive storytelling in a text shell. It covers creating and switching users, loading worlds by name or file, starting new story sessions, advancing choices, and viewing the narrative journal. The CLI leverages the orchestrator for all operations, which means even in the CLI, the engine's rules (like not advancing without a valid choice, etc.) are enforced consistently. The user experience is decent: upon making a choice, it prints the next block of story text and available next options, etc. The CLI is invaluable for quick internal testing and demoing the linear playback of stories. On the web/API side, the **REST API** is essentially complete for core interactions. Using FastAPI, it defines endpoints for all needed actions: creating a story (`POST /story/create`), getting the latest story update (`GET /update`), performing an action (`POST /do`), checking story status or dropping a story, plus separate routes for listing worlds, getting world info/media, and user auth actions. This REST design aligns with typical game backend APIs. Importantly, the REST API includes **concurrency control** by using an async lock per user -- so if a user issues two actions at nearly the same time, one will wait and not corrupt the state. This is a strong sign of production-minded design, preventing race conditions on a single user's ledger. The integration with the web client (Vue-based **WebTangl**) is also coming together: the web guide indicates the frontend expects data in terms of "JournalStoryUpdate" (blocks of narrative with actions) and uses the API's responses to render the story feed. We see that the current `/update` and `/do` endpoints are designed to support this: `/update` returns the latest fragments and available choices, which the frontend combines into its StoryFlow, and `/do` returns the fragments resulting from a choice along with a status, which the frontend can append to the feed. The **OpenAPI docs** generation via FastAPI is a plus -- developers can explore and test the API easily, and the types (like `ChoiceRequest` body) are clearly defined. This all suggests that an external developer (or the web client team) can work against the API without needing to know engine internals. The CLI and API also have tests ensuring basic flows work (integration tests in `engine/tests/integration/` likely cover starting a demo story, etc.), which adds confidence. In short, the application layer already proves that StoryTangl can be driven externally in both text and web contexts -- a big milestone for the project's usability.

**Gaps and Issues:** Because the application layer is closely tied to the service layer, most gaps there (like error formatting, access control) will surface here too -- those won't be repeated in detail. Focusing on the CLI/API specifics: one gap is the **lack of a rich output format in CLI**. Currently the CLI just prints raw text from fragments and likely lists choice texts. It might not show certain structured info like if a choice is unavailable (since that isn't provided by engine yet) or metadata like scene titles or other info fragments. As the engine adds features (like media or styled text), the CLI will need some capability to handle at least a basic representation (even if just "\[Image: dragon.png\]" placeholders or stripping Markdown to plain text). Another minor issue is that the CLI and REST might not yet handle **multi-user sessions** robustly. The CLI, for instance, keeps a current user in memory; if you wanted two users playing simultaneously in CLI, you'd have to manually switch users. That's fine for a dev tool. The REST API is stateless per request (auth via API key), which is good, but there's no notion of user logout or session timeout -- once a key, always that user. A related point: **administration and content management via CLI/API** is limited. For example, if you want to unload a world or list all loaded worlds, there are commands (WorldController.unload_world exists, list_worlds exists), but they might not be exposed in CLI (the CLI likely has a "worlds" command listing world names, etc., but maybe not unloading). The API has `/world/list` and `/world/load` but not a `/world/unload` route in what we saw -- though the controller has it, so it could be easily added. This is low priority (unloading might matter to free memory). Another consideration: **GraphQL or real-time API** is "future" -- currently everything is request/response. The design diagram alludes to GraphQL in the future, which could allow more flexible querying of state. If that's still a goal, none of it is started yet (and it doesn't need to be until the rest is stable). One issue mentioned earlier is **response consistency**: for instance, after `create_story`, the API returns a mixture of data -- in the code, it attempts to remove the raw Ledger object before returning. If any slip-up happens, the client might get an un-JSONable object. So tightening that (via BaseResponse) is needed to ensure clients never see internal objects or Python-specific representations. In terms of the web client, the current API forces it to do an extra step: merging fragments and choices. If we change the API to provide a pre-combined "story update", the web app will become simpler. So there's a bit of technical debt in the API design that was done to accommodate lacking engine features (like no nested fragments). We should address that once the engine can provide richer data. Finally, documentation of the CLI and API could improve. The CLI does have help text, but a new developer might benefit from a quickstart guide (e.g., "run `tangl` CLI, then `user new bob`, `world load demo.yaml`, etc."). For the API, beyond the auto docs, a few examples in the README or docs site (like how to authenticate and call /story/create and /do) would be helpful. These aren't code issues per se, but they affect the approachability of the application layer.

**Recommendations:** Many recommendations for this layer mirror those for the service layer (since improving response envelopes, error handling, etc., will directly benefit API users and CLI output). To reiterate a key one: implement a consistent **response schema** and apply it across CLI and API. For the CLI, that might mean the CLI client code should interpret the structured response from orchestrator instead of just dumping it. For example, if `resolve_choice` returns `{"success": false, "error": "Quest not found"}`, the CLI should detect success False and print a user-friendly error, whereas now it might just throw an exception or print a Python error. So updating the CLI's command handlers to handle the new response format will be necessary in tandem with service changes. Enhancing the CLI output formatting could also make it a better debugging tool: for instance, color-code different fragment types (maybe use green for dialog fragments, blue for choices, etc.) or indent choices under the block text to visually group them. Cmd2 supports colored output; this could be a nice quality-of-life improvement for developers using the CLI. On the API side, once the engine and service provide combined story updates (one block with its actions), update the `/update` and `/do` endpoints accordingly. Possibly deprecate `/update` in favor of something like `/story/journal` or `/story/update` that always returns the latest N blocks each with their actions and media. The current approach of separate fragments/choices lists can then be dropped. If breaking changes to the API are a concern for the existing web client, coordinate the changes -- but since both are evolving in tandem, now is the time to adjust. Implementing **unload_world** in the REST API (and a CLI command for it) is a quick win if memory management is needed (the controller method is there, just not exposed via route). Another recommendation is to start thinking about a **real-time feed** for the journal (though this might be out of scope for now). For example, using WebSockets to push new fragments to the client instead of the client polling /update. FastAPI could allow a WS endpoint that sends fragments as they're appended. This would make the web app more responsive (no need to poll each turn). It's not urgent given the turn-based nature of the game (player input drives updates), but if any asynchronous events or multi-user scenarios come into play, a push model might be beneficial. Regarding GraphQL: if there's interest, the existing API could be mapped to GraphQL relatively easily given the strong typing (GraphQL would be nice for queries like "get all quests" or complex state queries). That can be a medium-term exploration after REST is fully stabilized. Ensuring robust **testing** of the CLI and API is also recommended: use the CLI in automated tests (maybe via Pexpect or feeding commands) to ensure it behaves (especially after integrating new output formats). For the FastAPI side, the project already likely uses the TestClient to simulate calls -- expand those tests for error cases (e.g., invalid choice id, unauthorized access once auth is enforced, etc.). This will catch mismatches between what orchestrator returns and what the API expects to send. Finally, improve **documentation and developer guides**. The existence of the WebTangl contributor guide shows there's parallel documentation for the frontend; similarly, adding a section in StoryTangl's docs for "Using the CLI" and "Using the HTTP API" with examples will help internal teams and new contributors. This might include sample curl commands or CLI transcripts. All combined, these steps will elevate the application layer from a working demo to a polished interface. It will make the engine easier to integrate into other projects or to be used by non-core developers, which is crucial as the project grows beyond just the engine team.

## Scripting and Story Definition

**Completed and Working Well:** StoryTangl includes a powerful **scripting subsystem** that allows authors to define worlds and stories in human-readable data (YAML/JSON), which are then compiled into the engine's graph structures. The groundwork for this is solid. There is a well-defined Pydantic model schema for scripts -- the `StoryScript` class and related models (SceneScript, ActorScript, LocationScript, etc.) encapsulate the structure of a world definition. This schema covers scenes (with their blocks and optional roles/settings), global actors and locations, asset lists, and even templates for reusable entities. By using Pydantic, the script loader automatically validates and provides useful errors if the YAML is malformed, which is great for authoring feedback. The `ScriptManager` is the bridge from these static definitions to runtime -- it can load data from a file or dict, parse it into the Pydantic model, and then expose methods to iterate over the script content and create actual engine objects. The design of ScriptManager uses a clever **default class mapping** (`_default_tree`) that maps high-level script sections (like "actors", "scenes", "blocks") to the Python classes to instantiate (Actor, Scene, Block, etc.). This means authors can omit explicit class references in the script for standard elements, keeping scripts clean. ScriptManager infers, for example, that entries under "actors" should become `Actor` nodes in the graph, scenes become `Scene` nodes, and nested "blocks" under a scene become `Block` nodes, and so on. It also handles nesting: e.g., within a Block, "actions" or "continues" get defaulted to the `Action` class for choice edges. This mechanism is working (the engine successfully compiles provided example worlds like the demo). The **World** class (`tangl.story.fabula.world.World`) ties it together: when you load a script via `WorldController.load_world`, it creates a new World instance with the given ScriptManager. That World instance compiles the script into a **StoryGraph** internally. From there, calling `World.create_story()` instantiates a fresh Graph (StoryGraph) with all the defined scenes, blocks, etc. Tests indicate you can create multiple story sessions from one World (so a world is like a template). This separation is beneficial: it allows the same authored content to be replayed or used by many users independently. The scripting system also supports some dynamic features -- for instance, blocks can include inline Jinja2 in their text (which is rendered at runtime by ContentRenderer), so authors can put placeholders in narrative text. Also, the script can define **globals/locals** that become part of the story's state context (e.g., setting a global variable that can influence conditional logic). The fact that Requirements and Dependencies are first-class means the script can declare needs (like roles needing actors, or scenes requiring certain items) in a data way. This is advanced compared to most game engines where such logic might be hard-coded; here it's data-driven. The provided tests (`test_demo_script.py`, `test_role_provisioning.py`, etc.) show that you can write a small script and the engine correctly creates all corresponding nodes/edges, which is a strong proof that the compiler part is mostly correct. Overall, the scripting layer achieves its primary goal: to let non-programmers describe narrative content that the engine can run, without writing Python. It's also aligned with the **layering** -- scripts are purely data/definition (the "fabula" layer), which compile down into core graph constructs (scenes, nodes, edges in the "story" layer), and then the VM and service take over at runtime. This separation ensures that changes in narrative content don't require engine code changes, which is exactly what we want.

**Gaps and Issues:** Most of the issues in this layer revolve around incomplete integration of the scripting system with engine features (some of which we covered in the planning section). The highest priority one is that certain script-defined constructs are not yet hooked up to runtime logic. We already noted that **roles and settings in scripts are parsed but dropped on the floor** -- the World compiler creates Scene and Role objects (the Role class is a subtype of Dependency edge), but currently if a role has an `actor_ref` or template, that isn't leading to an actual actor being cast unless the actor exists independently. For example, in `test_role_provisioning.py`, defining a role \"king\" with an `actor_ref: "king_actor"` does result in a Role node with `actor_ref` in it, but unless the engine explicitly resolves it, it just stays as a placeholder with `actor is None`. The fix needs to happen during or right after `create_story` -- this is an integration hole between the fabula compilation and the VM planning (discussed earlier). Similarly, **templates in scripts** are not fully utilized: you can define a section of templates in the YAML, but as noted, the TemplateProvisioner doesn't fetch them, so a Requirement with `template_ref` will fail to provision. This is a disconnect between ScriptManager (which holds the templates dict from the script) and the provisioning system that needs that data. Another gap is **Media in scripts**: the media design doc defines `MediaScriptItem` with fields like `media_id`, `media_path` or `media_spec` to declare media attachments in the script, but it appears that the current StoryScript model does not include a top-level "media" section explicitly -- instead, media might be listed under blocks as in the example. It's unclear if the Pydantic models currently support parsing that (the docs mention *basic* MediaScriptItem schema is present, but full integration is TODO). So, adding media to a world script might not work out of the box yet. On a related note, **assets and traits**: The StoryScript model has an "assets" field (list of AssetScripts) presumably for defining item properties or inventory, etc. This part of the model might not yet feed into the engine anywhere -- possibly legacy from older design. It might be overlooked content that doesn't do much unless the engine explicitly uses it to create Item nodes or similar. Another potential issue: **Episode abstraction**. The engine has a concept of Episode nodes (maybe grouping scenes). If the design envisions "episodes" (like chapters) in scripts, the current YAML schema doesn't have an explicit episode section -- it jumps from world to scenes. The planning missing #7 suggests the engine hasn't implemented episode-level provisioning scope. So scripts can't define episodes yet, meaning everything is one big episode implicitly. This isn't a deal-breaker but is something to design in the future if needed (e.g., allow grouping scenes into episodes in YAML). From a tooling perspective, another gap is that **ScriptManager.from_files** is stubbed out -- right now to load from a file, the WorldController actually reads the file with `yaml.safe_load` and passes the data to `from_data`. This works, but if down the line worlds span multiple files or include statements, that's not handled. Also, error reporting during script loading could be friendlier -- Pydantic will pinpoint the location of validation issues, but if a script has a logic error (like a scene references an actor that isn't defined anywhere), the engine might not flag that clearly (except later during play when something is None). Some of those validations could be done at load time (e.g., cross-referencing that all actor_refs have a corresponding actor in the script). The system doesn't appear to do that yet. Another subtle point: **Ordering and IDs**. The script can use either lists or dicts for sections (thanks to the alias and validators that set label from dict keys). Using dicts is convenient because keys become labels, but Python dicts (in 3.7+) preserve insertion order, so likely the play order of scenes/blocks is deterministic. However, authors might wonder if they can rely on that order or if they should use an explicit "order" field or list. It seems fine as is, but documentation should clarify it. Finally, test coverage shows a scenario with roles, but perhaps not many complex scenarios (like multiple requirements, or clones). That suggests that while the pieces exist, not all have been exercised in combination -- there may be edge cases lurking (for example, a role with both an `actor_ref` and an `actor_template` might double-provision an actor or neither, depending on how Requirement is constructed). The Role model's validator sets up the Requirement with those fields, but the actual effect depends on planning executing properly. So there's a bit of uncertainty until those features are fully tested with integrated planning.

**Recommendations:** To strengthen the scripting layer, the main effort is to **complete the loop between script definitions and runtime behavior**. Start with the straightforward fix: when compiling a world, for each scene's roles and settings, immediately create the corresponding open edges (Dependency edges) pointing to nothing, with the requirement filled out (as shown in the planning doc snippet). This might be done in `World.create_story` after the scenes and roles are created, or even in the `SceneScript` model via a post-init (but doing it in World is more explicit). Once roles have proper Requirement objects and dependency edges, the planner will handle them on the first cycle. Next, wire up the **template registry**: after parsing a script, the ScriptManager holds `master_script.templates`. We should pass that into the World or Orchestrator such that when provisioning happens, the TemplateProvisioner can access it. One design is to store the template dict on the World object (since World is a singleton for that content). In `Orchestrator.execute`, when it creates a Frame, perhaps inject the World's template mapping into the Frame's context or the ProvisioningContext. Alternatively, simpler, when constructing Requirements from script, if a requirement has a `template_ref`, do a lookup in ScriptManager at that time and copy the template inline (so the Requirement ends up with a concrete template dict). This would avoid needing TemplateProvisioner to do the lookup later. Either approach works; the latter might be easier and ensures that by the time planning runs, every Requirement either has a concrete template or not. Don't forget to adjust tests or add new ones for this (e.g., define a template in script and require it, then check that a node with that template gets created). For **media in scripts**, extend the StoryScript model to allow media attachments on blocks or scenes (if not already). Then, during compilation, for each media entry, create a MediaDependency edge on the relevant block pointing to None with a MediaRequirement (similar to roles). This is analogous to roles, just different type. The media design doc's example shows how a block in YAML could have a media list -- we need to ensure that gets parsed (so probably add a `media: list[MediaScriptItem]` field to BlockScript model) and then processed. This will let the planning system provision media at the right time and, once the journaling part is in place, output it. Improving **validation** at compile time: consider adding checks in ScriptManager or World.create_story for common mistakes, like missing references. For example, after parsing, we could verify that every role's `actor_ref` corresponds to either an actor defined in the script or an actor that will be provisioned (if it's meant to reference an external concept). If something's clearly wrong (reference to undefined actor), warn or error early. Pydantic could also be used for this via `@model_validator` on StoryScript (once the whole model is populated) to cross-validate sections. Similarly, ensure unique labels -- the script uses labels as keys, but if someone uses list form and accidentally duplicates a label field, it might currently pass (depending on Pydantic enforcement). We might enforce uniqueness of labels within each category. Another recommendation: develop a **library of example scripts** and use them for testing and documentation. For instance, a "hello world" script that exercises roles, a script that uses an inline template, one that attaches media, etc. Running these through the engine in tests will reveal if any part of the pipeline is not connected. It will also serve as documentation for authors. On the authoring side, eventually providing tooling or at least documentation on the YAML format is needed -- but presumably that's planned via the docs (there's likely a section in Sphinx docs for writing worlds, though we haven't cited it here). One more medium-term idea: currently script logic that is more procedural (like "if flag X do this scene else that scene") isn't directly expressible except by using conditions on edges or writing custom handler code. In the future, there might be desire for a lightweight scripting (like a Python or Lua snippet in the YAML). This is a slippery slope, but if it comes up, ensure it's sandboxed (perhaps via the existing expression evaluation with `ctx.eval_expr`, which might already allow some arithmetic or randomness). The current approach favors data-driven logic (via tags and conditions), which is good to keep for determinism. Finally, consider exposing some of the scripting functionality via the service layer for dynamic story generation. For example, an admin API to load a new world from a given YAML payload is already there (`load_world` takes a `script_data`). The inverse (exporting a world back to YAML) isn't implemented but could be handy (though not trivial, since once dynamic content is added, it's not purely the original script). Not urgent, but a thought for tooling completeness. In summary, the scripting layer mainly needs those integration touch-ups (roles, templates, media) and more testing. Given those, StoryTangl will truly allow complex worlds to be written declaratively and run correctly -- fulfilling the promise of separating narrative content from engine code and making the system accessible to content designers.
